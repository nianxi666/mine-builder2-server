#!/usr/bin/env python3
"""
DiTè®­ç»ƒè„šæœ¬ - å¸¦æ¯Næ­¥æ¨ç†åŠŸèƒ½
ä¿å­˜æ‰€æœ‰æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
"""

import argparse
import json
import os
import time
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# å¯¼å…¥æ¨¡å‹å’Œæ¨ç†
from dit_model import DiT3D
from inference import DDIMSampler, DDPMSampler


class VoxelDataset(Dataset):
    """16x16x16ä½“ç´ æ•°æ®é›†"""
    
    def __init__(self, dataset_dir: str):
        self.dataset_dir = Path(dataset_dir)
        self.samples = sorted(list(self.dataset_dir.glob("sample_*")))
        
        if len(self.samples) == 0:
            raise ValueError(f"No samples found in {dataset_dir}")
        
        print(f"Found {len(self.samples)} samples")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample_dir = self.samples[idx]
        npz_file = sample_dir / "voxels.npz"
        
        # åŠ è½½NPZ
        data = np.load(npz_file, allow_pickle=True)
        voxels = torch.from_numpy(data['voxels']).float()  # (16,16,16,2)
        prompt = str(data.get('prompt', ''))
        
        # è½¬æ¢ä¸º(2,16,16,16)
        voxels = voxels.permute(3, 0, 1, 2)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        voxels = voxels / 127.5 - 1.0
        
        return voxels, prompt


def save_inference_result(voxel_array, output_path, step, sample_idx):
    """ä¿å­˜æ¨ç†ç»“æœ"""
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åå½’ä¸€åŒ–
    voxel_array = ((voxel_array + 1.0) * 127.5).clamp(0, 255).cpu().numpy()
    voxel_array = voxel_array.astype(np.int16)
    
    # (2, 16, 16, 16) -> (16, 16, 16, 2)
    voxel_array = voxel_array.transpose(1, 2, 3, 0)
    
    # ä¿å­˜JSON
    voxels_list = []
    for x in range(16):
        for y in range(16):
            for z in range(16):
                block_id = int(voxel_array[x, y, z, 0])
                meta_data = int(voxel_array[x, y, z, 1])
                if block_id > 0:
                    voxels_list.append({
                        "x": int(x),
                        "y": int(y),
                        "z": int(z),
                        "block_id": block_id,
                        "meta_data": meta_data
                    })
    
    result = {
        "structure_name": f"inference_step_{step}_sample_{sample_idx}",
        "description": f"Generated at training step {step}",
        "voxels": voxels_list
    }
    
    json_file = output_path / f"step_{step:06d}_sample_{sample_idx}.json"
    with open(json_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    return json_file


def inference_during_training(model, device, output_dir, step, num_samples=2, num_steps=10):
    """è®­ç»ƒä¸­æ¨ç†"""
    model.eval()
    
    sampler = DDIMSampler(model, num_steps=num_steps)
    
    print(f"\nğŸ¨ Step {step}: ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")
    
    with torch.no_grad():
        for i in range(num_samples):
            # ç”Ÿæˆ
            sample = sampler.sample(
                shape=(2, 16, 16, 16),
                device=device
            )
            
            # ä¿å­˜
            save_inference_result(
                sample[0],
                output_dir,
                step,
                i
            )
    
    model.train()
    print(f"âœ… æ¨ç†å®Œæˆï¼Œä¿å­˜åˆ° {output_dir}")


def train(args):
    """è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®è¾“å‡ºç›®å½• - æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨/gemini/code
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = output_dir / "checkpoints"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    inference_dir = output_dir / "inference_samples"
    inference_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½æ•°æ®é›†: {args.dataset_dir}")
    dataset = VoxelDataset(args.dataset_dir)
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    # æ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹: {args.model_size}")
    if args.model_size == 'small':
        model = DiT3D(hidden_size=384, depth=12, num_heads=6)
    elif args.model_size == 'base':
        model = DiT3D(hidden_size=768, depth=12, num_heads=12)
    elif args.model_size == 'large':
        model = DiT3D(hidden_size=1024, depth=24, num_heads=16)
    else:
        raise ValueError(f"Unknown model size: {args.model_size}")
    
    model = model.to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒ: {trainable_params:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    # AMP
    scaler = torch.cuda.amp.GradScaler() if args.use_amp and torch.cuda.is_available() else None
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   Epochs: {args.epochs}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Learning rate: {args.lr}")
    print(f"   æ¯ {args.inference_every} æ­¥æ¨ç†ä¸€æ¬¡")
    print("=" * 60)
    
    global_step = 0
    start_time = time.time()
    
    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.epochs}")
        
        for batch_idx, (voxels, prompts) in enumerate(pbar):
            voxels = voxels.to(device)
            
            # éšæœºæ—¶é—´æ­¥
            batch_size = voxels.shape[0]
            t = torch.randint(0, 1000, (batch_size,), device=device)
            
            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(voxels)
            alpha = (1 - t.float() / 1000).view(-1, 1, 1, 1, 1)
            noisy_voxels = alpha.sqrt() * voxels + (1 - alpha).sqrt() * noise
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=(scaler is not None)):
                pred_noise = model(noisy_voxels, t)
                loss = nn.functional.mse_loss(pred_noise, noise)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()
            
            epoch_loss += loss.item()
            global_step += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'step': global_step
            })
            
            # æ¯Næ­¥æ¨ç†ä¸€æ¬¡
            if global_step % args.inference_every == 0:
                inference_during_training(
                    model,
                    device,
                    inference_dir,
                    global_step,
                    num_samples=args.inference_samples,
                    num_steps=args.inference_steps
                )
            
            # ä¿å­˜checkpoint
            if global_step % args.save_every == 0:
                checkpoint_path = checkpoint_dir / f"step_{global_step:06d}.pt"
                torch.save({
                    'step': global_step,
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss.item(),
                }, checkpoint_path)
                print(f"ğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path}")
        
        avg_loss = epoch_loss / len(dataloader)
        elapsed = time.time() - start_time
        
        print(f"Epoch {epoch+1}/{args.epochs} - Loss: {avg_loss:.4f} - Time: {elapsed/60:.1f}min")
        
        # ä¿å­˜æœ€æ–°
        latest_path = checkpoint_dir / "latest.pt"
        torch.save({
            'step': global_step,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, latest_path)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ€»æ­¥æ•°: {global_step}")
    print(f"   æ€»æ—¶é—´: {(time.time() - start_time)/3600:.2f} å°æ—¶")
    print(f"   Checkpoints: {checkpoint_dir}")
    print(f"   æ¨ç†æ ·æœ¬: {inference_dir}")


def main():
    parser = argparse.ArgumentParser(description="DiTè®­ç»ƒ - å¸¦æ¨ç†æµ‹è¯•")
    
    # æ•°æ®
    parser.add_argument('--dataset-dir', type=str, default='/gemini/code/dataset',
                        help='æ•°æ®é›†ç›®å½•')
    parser.add_argument('--output-dir', type=str, default='/gemini/code/outputs',
                        help='è¾“å‡ºç›®å½•ï¼ˆæ‰€æœ‰æ–‡ä»¶éƒ½ä¿å­˜è¿™é‡Œï¼‰')
    
    # æ¨¡å‹
    parser.add_argument('--model-size', type=str, default='small',
                        choices=['small', 'base', 'large'],
                        help='æ¨¡å‹å¤§å°')
    
    # è®­ç»ƒ
    parser.add_argument('--batch-size', type=int, default=8,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--epochs', type=int, default=50,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--num-workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--use-amp', action='store_true',
                        help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    
    # ä¿å­˜å’Œæ¨ç†
    parser.add_argument('--save-every', type=int, default=100,
                        help='æ¯Næ­¥ä¿å­˜ä¸€æ¬¡checkpoint')
    parser.add_argument('--inference-every', type=int, default=5,
                        help='æ¯Næ­¥æ¨ç†ä¸€æ¬¡')
    parser.add_argument('--inference-samples', type=int, default=2,
                        help='æ¨ç†æ—¶ç”Ÿæˆçš„æ ·æœ¬æ•°')
    parser.add_argument('--inference-steps', type=int, default=10,
                        help='æ¨ç†æ—¶çš„é‡‡æ ·æ­¥æ•°')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("DiT Minecraft ä½“ç´ ç”Ÿæˆ - è®­ç»ƒ")
    print("=" * 60)
    print(f"é…ç½®:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("=" * 60)
    
    train(args)


if __name__ == "__main__":
    main()
