# ğŸš€ è¿œç¨‹GPUæœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

## ğŸ“‹ æœåŠ¡å™¨ä¿¡æ¯

```bash
SSH: ssh -p 30022 root@ssh-06aace8d2b864475e7b3e2eb54a8107e.zlrast8j3bxb@direct.virtaicloud.com
å†…å­˜: 16GB
æ˜¾å­˜: 6GB (GTX 1060 æˆ–ç±»ä¼¼)
å·¥ä½œç›®å½•: /gemini/code
```

## ğŸ¯ å®Œæ•´éƒ¨ç½²æµç¨‹

### æ­¥éª¤1: ä¸Šä¼ ä»£ç åŒ…

åœ¨**æœ¬åœ°æœºå™¨**æ‰§è¡Œï¼š

```bash
cd /home/engine/project

# å·²ç»æ‰“åŒ…å¥½äº†
ls -lh minecraft_dit.tar.gz  # åº”è¯¥çœ‹åˆ°40KBçš„æ–‡ä»¶

# ä¸Šä¼ åˆ°è¿œç¨‹æœåŠ¡å™¨
scp -P 30022 minecraft_dit.tar.gz \
  root@ssh-06aace8d2b864475e7b3e2eb54a8107e.zlrast8j3bxb@direct.virtaicloud.com:/gemini/code/
```

### æ­¥éª¤2: SSHè¿æ¥åˆ°æœåŠ¡å™¨

```bash
ssh -p 30022 root@ssh-06aace8d2b864475e7b3e2eb54a8107e.zlrast8j3bxb@direct.virtaicloud.com
```

### æ­¥éª¤3: åœ¨æœåŠ¡å™¨ä¸Šè§£å‹å¹¶è®¾ç½®ç¯å¢ƒ

```bash
# è¿›å…¥å·¥ä½œç›®å½•
cd /gemini/code

# è§£å‹
tar -xzf minecraft_dit.tar.gz
cd upload_package

# æ£€æŸ¥GPU
nvidia-smi

# æ£€æŸ¥Pythonç‰ˆæœ¬
python3 --version

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–ï¼ˆæ ¹æ®6GBæ˜¾å­˜ä¼˜åŒ–ï¼‰
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install google-generativeai numpy tqdm

# æµ‹è¯•PyTorchå’ŒCUDA
python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
```

### æ­¥éª¤4: ç”Ÿæˆæ•°æ®é›†ï¼ˆåœ¨æœåŠ¡å™¨ä¸Šï¼‰

ç”±äºæœåŠ¡å™¨æ€§èƒ½æ›´å¥½ï¼Œç›´æ¥åœ¨æœåŠ¡å™¨ä¸Šç”Ÿæˆæ•°æ®é›†ï¼š

```bash
# ä½¿ç”¨screenæŒ‚èµ·ä¼šè¯
screen -S dataset_gen

# ç”Ÿæˆ1000ä¸ªæ ·æœ¬ï¼ˆçº¦12-20å°æ—¶ï¼‰
python3 generate_premium_dataset.py \
  --api-key AIzaSyB3xn379AZKVmCEIywishHGo_57GDj1o9A \
  --num-samples 1000 \
  --output-dir dataset

# æŒ‰ Ctrl+A+D æŒ‚èµ·screen
# æŸ¥çœ‹è¿›åº¦: screen -r dataset_gen
```

**æˆ–è€…å¿«é€Ÿæµ‹è¯•ï¼ˆç”Ÿæˆ50ä¸ªæ ·æœ¬ï¼‰ï¼š**

```bash
screen -S dataset_test

python3 generate_premium_dataset.py \
  --api-key AIzaSyB3xn379AZKVmCEIywishHGo_57GDj1o9A \
  --num-samples 50 \
  --output-dir dataset_test

# Ctrl+A+D æŒ‚èµ·
```

### æ­¥éª¤5: å¼€å§‹è®­ç»ƒï¼ˆ6GBæ˜¾å­˜ä¼˜åŒ–é…ç½®ï¼‰

**é‡è¦ï¼š6GBæ˜¾å­˜éœ€è¦å°å¿ƒé…ç½®ï¼**

#### æ–¹æ¡ˆA: Smallæ¨¡å‹ + å°batchï¼ˆæœ€å®‰å…¨ï¼‰

```bash
screen -S training

python3 train.py \
  --dataset-dir dataset \
  --output-dir outputs \
  --model-size small \
  --batch-size 2 \
  --epochs 100 \
  --lr 1e-4 \
  --use-amp \
  --num-workers 2

# Ctrl+A+D æŒ‚èµ·
# æŸ¥çœ‹: screen -r training
```

#### æ–¹æ¡ˆB: Smallæ¨¡å‹ + ä¸­batchï¼ˆæ¨èï¼‰

```bash
python3 train.py \
  --dataset-dir dataset \
  --output-dir outputs \
  --model-size small \
  --batch-size 4 \
  --epochs 150 \
  --lr 1e-4 \
  --use-amp \
  --num-workers 4
```

#### å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè°ƒæ•´å‚æ•°ï¼š

```bash
# æœ€å°é…ç½®
python3 train.py \
  --dataset-dir dataset \
  --model-size small \
  --batch-size 1 \
  --epochs 100 \
  --use-amp
```

### æ­¥éª¤6: ç›‘æ§è®­ç»ƒ

#### æŸ¥çœ‹è®­ç»ƒè¿›åº¦

```bash
# åˆ—å‡ºæ‰€æœ‰screenä¼šè¯
screen -ls

# æ¢å¤è®­ç»ƒä¼šè¯
screen -r training

# æŸ¥çœ‹æ—¥å¿—ï¼ˆå¦‚æœä½¿ç”¨nohupï¼‰
tail -f training.log

# æŸ¥çœ‹GPUä½¿ç”¨
watch -n 1 nvidia-smi
```

#### æ£€æŸ¥checkpoints

```bash
ls -lh outputs/checkpoints/
cat outputs/config.json
```

### æ­¥éª¤7: è®­ç»ƒå®Œæˆåæ¨ç†

```bash
screen -S inference

python3 inference.py \
  --checkpoint outputs/checkpoints/latest.pt \
  --model-size small \
  --num-samples 20 \
  --sampler ddim \
  --num-steps 50 \
  --output-dir generated

# Ctrl+A+D æŒ‚èµ·
```

## ğŸ“Š Screenå‘½ä»¤é€ŸæŸ¥

```bash
# åˆ›å»ºæ–°ä¼šè¯
screen -S ä¼šè¯å

# åˆ—å‡ºæ‰€æœ‰ä¼šè¯
screen -ls

# æ¢å¤ä¼šè¯
screen -r ä¼šè¯å

# æŒ‚èµ·ä¼šè¯ï¼ˆåœ¨screenå†…ï¼‰
Ctrl+A+D

# æ€æ­»ä¼šè¯
screen -X -S ä¼šè¯å quit

# æŸ¥çœ‹æ‰€æœ‰çª—å£
Ctrl+A+"
```

## ğŸ”§ å†…å­˜/æ˜¾å­˜ä¸å¤Ÿæ—¶çš„è§£å†³æ–¹æ¡ˆ

### å¦‚æœè®­ç»ƒæ—¶OOMï¼ˆæ˜¾å­˜ä¸è¶³ï¼‰

1. **å‡å°batch size**
```bash
--batch-size 1  # æœ€å°
```

2. **ä½¿ç”¨æ··åˆç²¾åº¦**
```bash
--use-amp  # å¿…é¡»å¯ç”¨
```

3. **å‡å°‘workers**
```bash
--num-workers 1
```

4. **ç›‘æ§æ˜¾å­˜ä½¿ç”¨**
```bash
watch -n 0.5 nvidia-smi
```

### å¦‚æœå†…å­˜ä¸å¤Ÿï¼ˆ16GBåº”è¯¥å¤Ÿç”¨ï¼‰

```bash
# å‡å°‘æ•°æ®åŠ è½½workers
--num-workers 2

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -h
htop
```

### æ˜¾å­˜ä½¿ç”¨é¢„ä¼°

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨ |
|------|----------|------|
| Small, batch=1 | ~2GB | âœ… 6GBå¤Ÿç”¨ |
| Small, batch=2 | ~3GB | âœ… 6GBå¤Ÿç”¨ |
| Small, batch=4 | ~4-5GB | âœ… 6GBå¤Ÿç”¨ï¼ˆç´§å¼ ï¼‰|
| Small, batch=8 | ~6-7GB | âŒ 6GBä¸å¤Ÿ |
| Base, batch=1 | ~4GB | âš ï¸ å¯èƒ½ç´§å¼  |

## ğŸ¨ ä½¿ç”¨åˆæˆæ•°æ®å¿«é€Ÿæµ‹è¯•ï¼ˆæ— éœ€APIï¼‰

å¦‚æœæƒ³å¿«é€Ÿå¼€å§‹ï¼Œå¯ä»¥ä½¿ç”¨åˆæˆæ•°æ®ï¼š

```bash
# ç”Ÿæˆ100ä¸ªåˆæˆæ ·æœ¬ï¼ˆç§’çº§å®Œæˆï¼‰
python3 create_synthetic_dataset.py \
  --num-samples 100 \
  --output-dir dataset_synthetic

# å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰
python3 train.py \
  --dataset-dir dataset_synthetic \
  --model-size small \
  --batch-size 4 \
  --epochs 10 \
  --use-amp
```

## ğŸ“ˆ é¢„æœŸæ—¶é—´

### æ•°æ®é›†ç”Ÿæˆï¼ˆä½¿ç”¨Gemini APIï¼‰
- 50ä¸ªæ ·æœ¬: ~2-3å°æ—¶
- 100ä¸ªæ ·æœ¬: ~4-6å°æ—¶
- 1000ä¸ªæ ·æœ¬: ~12-20å°æ—¶

### è®­ç»ƒæ—¶é—´ï¼ˆ6GBæ˜¾å­˜ï¼ŒSmallæ¨¡å‹ï¼‰
- 10 epochs: ~30åˆ†é’Ÿ
- 50 epochs: ~2-3å°æ—¶
- 100 epochs: ~4-6å°æ—¶
- 200 epochs: ~8-12å°æ—¶

### æ¨ç†æ—¶é—´
- DDIM 50æ­¥: ~10ç§’/æ ·æœ¬
- DDPM 1000æ­¥: ~3-5åˆ†é’Ÿ/æ ·æœ¬

## ğŸ¯ æ¨èå·¥ä½œæµ

### å¿«é€Ÿæµ‹è¯•æµç¨‹ï¼ˆ1å°æ—¶ï¼‰

```bash
# 1. åˆæˆæ•°æ®ï¼ˆ1åˆ†é’Ÿï¼‰
python3 create_synthetic_dataset.py --num-samples 100

# 2. å¿«é€Ÿè®­ç»ƒï¼ˆ30åˆ†é’Ÿï¼‰
screen -S test_train
python3 train.py --dataset-dir dataset_synthetic --epochs 30 --batch-size 4 --use-amp
# Ctrl+A+D

# 3. æ¨ç†æµ‹è¯•ï¼ˆ5åˆ†é’Ÿï¼‰
python3 inference.py --checkpoint outputs/checkpoints/latest.pt --num-samples 10
```

### å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆ24-48å°æ—¶ï¼‰

```bash
# 1. ç”Ÿæˆé«˜è´¨é‡æ•°æ®é›†ï¼ˆ12-20å°æ—¶ï¼‰
screen -S dataset
python3 generate_premium_dataset.py --num-samples 1000
# Ctrl+A+D

# ç­‰å¾…å®Œæˆå...

# 2. å®Œæ•´è®­ç»ƒï¼ˆ8-12å°æ—¶ï¼‰
screen -S training
python3 train.py --dataset-dir dataset --epochs 100 --batch-size 4 --use-amp
# Ctrl+A+D

# 3. ç”Ÿæˆç»“æœ
python3 inference.py --checkpoint outputs/checkpoints/latest.pt --num-samples 50
```

## ğŸ”¥ ä¸€é”®å¯åŠ¨è„šæœ¬

åˆ›å»ºå¿«é€Ÿå¯åŠ¨è„šæœ¬ï¼š

```bash
cat > quick_train.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸš€ å¯åŠ¨Minecraft DiTè®­ç»ƒ..."

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# æ£€æŸ¥æ•°æ®é›†
if [ ! -d "dataset/sample_0000" ]; then
    echo "ğŸ“¦ ç”Ÿæˆåˆæˆæ•°æ®é›†..."
    python3 create_synthetic_dataset.py --num-samples 100
fi

# å¼€å§‹è®­ç»ƒ
echo "ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ..."
python3 train.py \
    --dataset-dir dataset \
    --output-dir outputs \
    --model-size small \
    --batch-size 4 \
    --epochs 50 \
    --use-amp \
    --num-workers 2

echo "âœ… è®­ç»ƒå®Œæˆï¼"
EOF

chmod +x quick_train.sh

# åœ¨screenä¸­è¿è¡Œ
screen -S training ./quick_train.sh
```

## ğŸ†˜ å¸¸è§é—®é¢˜

### 1. SSHè¿æ¥æ–­å¼€æ€ä¹ˆåŠï¼Ÿ
ä½¿ç”¨screenæŒ‚èµ·ä¼šè¯ï¼Œé‡æ–°è¿æ¥åç”¨`screen -r`æ¢å¤

### 2. æ˜¾å­˜OOM
å‡å°batch_sizeåˆ°1æˆ–2

### 3. è®­ç»ƒlossä¸ä¸‹é™
- æ£€æŸ¥æ•°æ®é›†è´¨é‡
- å¢åŠ epochs
- è°ƒæ•´å­¦ä¹ ç‡

### 4. APIé™æµ
- è„šæœ¬ä¼šè‡ªåŠ¨é‡è¯•
- æˆ–ä½¿ç”¨åˆæˆæ•°æ®é›†

### 5. æŸ¥çœ‹è®­ç»ƒè¿›åº¦
```bash
screen -r training  # å®æ—¶æŸ¥çœ‹
# æˆ–è€…
tail -f outputs/train.log  # å¦‚æœæœ‰æ—¥å¿—
```

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥`screen -ls`çœ‹è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
2. æŸ¥çœ‹`nvidia-smi`æ£€æŸ¥GPUä½¿ç”¨
3. æŸ¥çœ‹`free -h`æ£€æŸ¥å†…å­˜
4. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶

---

**å‡†å¤‡å°±ç»ªï¼ç°åœ¨å¯ä»¥å¼€å§‹éƒ¨ç½²äº†ï¼** ğŸš€

å»ºè®®æµç¨‹ï¼š
1. ä¸Šä¼ ä»£ç åŒ… (scpå‘½ä»¤)
2. SSHè¿æ¥æœåŠ¡å™¨
3. è§£å‹å¹¶å®‰è£…ä¾èµ–
4. ä½¿ç”¨screenå¯åŠ¨æ•°æ®é›†ç”Ÿæˆ
5. æ•°æ®é›†å®Œæˆåå¯åŠ¨è®­ç»ƒ
6. å®šæœŸæ£€æŸ¥è¿›åº¦
