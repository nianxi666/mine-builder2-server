#!/usr/bin/env python3
"""
å¢é‡è®­ç»ƒè„šæœ¬ - åŠ¨æ€åŠ è½½æ–°æ•°æ®
å½“æ£€æµ‹åˆ°æ–°çš„æ•°æ®æ ·æœ¬æ—¶ï¼Œè‡ªåŠ¨æ·»åŠ åˆ°è®­ç»ƒé›†å¹¶ç»§ç»­è®­ç»ƒ
"""

import argparse
import json
import os
import time
from pathlib import Path
import shutil

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from dit_model import DiT3D
from inference import DDIMSampler


class IncrementalVoxelDataset(Dataset):
    """åŠ¨æ€å¢é‡åŠ è½½çš„ä½“ç´ æ•°æ®é›†"""
    
    def __init__(self, dataset_dir: str, min_samples: int = 10):
        self.dataset_dir = Path(dataset_dir)
        self.min_samples = min_samples
        self.samples = []
        self.last_check = 0
        self.refresh()
        
        if len(self.samples) < min_samples:
            print(f"âš ï¸  å½“å‰åªæœ‰{len(self.samples)}ä¸ªæ ·æœ¬ï¼Œç­‰å¾…è‡³å°‘{min_samples}ä¸ª...")
    
    def refresh(self):
        """åˆ·æ–°å¯ç”¨æ ·æœ¬åˆ—è¡¨"""
        current_samples = sorted(list(self.dataset_dir.glob("sample_*")))
        
        # åªæ·»åŠ æ–°æ ·æœ¬
        new_samples = [s for s in current_samples if s not in self.samples]
        
        if new_samples:
            self.samples.extend(new_samples)
            print(f"\nğŸ“Š å‘ç°{len(new_samples)}ä¸ªæ–°æ ·æœ¬ï¼æ€»è®¡ï¼š{len(self.samples)}ä¸ª")
        
        self.last_check = time.time()
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample_dir = self.samples[idx]
        npz_file = sample_dir / "voxels.npz"
        
        # åŠ è½½NPZ
        data = np.load(npz_file, allow_pickle=True)
        voxels = torch.from_numpy(data['voxels']).float()  # (16,16,16,2)
        prompt = str(data.get('prompt', ''))
        
        # è½¬æ¢ä¸º(2,16,16,16)
        voxels = voxels.permute(3, 0, 1, 2)
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]
        voxels = voxels / 127.5 - 1.0
        
        return voxels, prompt


def save_inference_result(voxel_array, output_path, step, sample_idx):
    """ä¿å­˜æ¨ç†ç»“æœ"""
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # åå½’ä¸€åŒ–
    voxel_array = ((voxel_array + 1.0) * 127.5).clamp(0, 255).cpu().numpy()
    voxel_array = voxel_array.astype(np.int16)
    
    # (2, 16, 16, 16) -> (16, 16, 16, 2)
    voxel_array = voxel_array.transpose(1, 2, 3, 0)
    
    # ä¿å­˜JSON
    voxels_list = []
    for x in range(16):
        for y in range(16):
            for z in range(16):
                block_id = int(voxel_array[x, y, z, 0])
                meta_data = int(voxel_array[x, y, z, 1])
                if block_id > 0:
                    voxels_list.append({
                        "x": int(x),
                        "y": int(y),
                        "z": int(z),
                        "block_id": block_id,
                        "meta_data": meta_data
                    })
    
    result = {
        "structure_name": f"inference_step_{step}_sample_{sample_idx}",
        "description": f"Generated at training step {step}",
        "voxels": voxels_list
    }
    
    json_file = output_path / f"step_{step:06d}_sample_{sample_idx}.json"
    with open(json_file, 'w') as f:
        json.dump(result, f, indent=2)
    
    return json_file


def inference_during_training(model, device, output_dir, step, num_samples=2, num_steps=50):
    """è®­ç»ƒä¸­æ¨ç†"""
    model.eval()
    sampler = DDIMSampler()
    
    print(f"\nğŸ¨ Step {step}: ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")
    
    with torch.no_grad():
        for i in range(num_samples):
            sample = sampler.sample(
                model=model,
                shape=(1, 2, 16, 16, 16),
                device=device,
                num_steps=num_steps
            )
            
            save_inference_result(
                sample[0],
                output_dir,
                step,
                i
            )
    
    model.train()
    print(f"âœ… æ¨ç†å®Œæˆ")


def train(args):
    """å¢é‡è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = output_dir / "checkpoints"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    inference_dir = output_dir / "inference_samples"
    inference_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # åˆ›å»ºå¢é‡æ•°æ®é›†
    print(f"\nğŸ“¦ ç›‘æ§æ•°æ®é›†: {args.dataset_dir}")
    dataset = IncrementalVoxelDataset(args.dataset_dir, min_samples=args.min_samples)
    
    # ç­‰å¾…æœ€å°æ•°é‡çš„æ ·æœ¬
    while len(dataset) < args.min_samples:
        print(f"ç­‰å¾…æ•°æ®... å½“å‰: {len(dataset)}/{args.min_samples}")
        time.sleep(10)
        dataset.refresh()
    
    print(f"âœ… åˆå§‹æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
    
    # æ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹: {args.model_size}")
    if args.model_size == 'small':
        model = DiT3D(hidden_size=384, depth=12, num_heads=6)
    elif args.model_size == 'base':
        model = DiT3D(hidden_size=768, depth=12, num_heads=12)
    elif args.model_size == 'large':
        model = DiT3D(hidden_size=1024, depth=24, num_heads=16)
    else:
        raise ValueError(f"Unknown model size: {args.model_size}")
    
    model = model.to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ€»å‚æ•°: {total_params:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=0.01,
        betas=(0.9, 0.999)
    )
    
    # AMP
    scaler = torch.cuda.amp.GradScaler() if args.use_amp and torch.cuda.is_available() else None
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nğŸš€ å¼€å§‹å¢é‡è®­ç»ƒ...")
    print(f"   æœ€å¤§Epochs: {args.max_epochs}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   æ•°æ®åˆ·æ–°é—´éš”: {args.refresh_interval}ç§’")
    print(f"   æ¯ {args.inference_every} æ­¥æ¨ç†ä¸€æ¬¡")
    print("=" * 60)
    
    global_step = 0
    start_time = time.time()
    last_dataset_size = len(dataset)
    
    for epoch in range(args.max_epochs):
        # åˆ·æ–°æ•°æ®é›†
        dataset.refresh()
        
        # é‡æ–°åˆ›å»ºDataLoaderï¼ˆå¦‚æœæœ‰æ–°æ•°æ®ï¼‰
        if len(dataset) > last_dataset_size:
            print(f"\nğŸ”„ æ•°æ®é›†æ›´æ–°ï¼ä»{last_dataset_size}ä¸ªå¢åŠ åˆ°{len(dataset)}ä¸ªæ ·æœ¬")
            last_dataset_size = len(dataset)
        
        dataloader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            shuffle=True,
            num_workers=min(args.num_workers, len(dataset)),
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        if len(dataloader) == 0:
            print(f"âš ï¸  Epoch {epoch+1}: æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œç­‰å¾…...")
            time.sleep(args.refresh_interval)
            continue
        
        model.train()
        epoch_loss = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.max_epochs} (æ ·æœ¬:{len(dataset)})")
        
        for batch_idx, (voxels, prompts) in enumerate(pbar):
            voxels = voxels.to(device)
            
            # éšæœºæ—¶é—´æ­¥
            batch_size = voxels.shape[0]
            t = torch.randint(0, 1000, (batch_size,), device=device)
            
            # æ·»åŠ å™ªå£°
            noise = torch.randn_like(voxels)
            alpha = (1 - t.float() / 1000).view(-1, 1, 1, 1, 1)
            noisy_voxels = alpha.sqrt() * voxels + (1 - alpha).sqrt() * noise
            
            # å‰å‘ä¼ æ’­
            with torch.cuda.amp.autocast(enabled=(scaler is not None)):
                pred_noise = model(noisy_voxels, t)
                loss = nn.functional.mse_loss(pred_noise, noise)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()
            
            epoch_loss += loss.item()
            global_step += 1
            
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'step': global_step,
                'samples': len(dataset)
            })
            
            # æ¯Næ­¥æ¨ç†ä¸€æ¬¡
            if args.inference_every > 0 and global_step % args.inference_every == 0:
                inference_during_training(
                    model,
                    device,
                    inference_dir,
                    global_step,
                    num_samples=args.inference_samples,
                    num_steps=args.inference_steps
                )
            
            # ä¿å­˜checkpoint
            if global_step % args.save_every == 0:
                checkpoint_path = checkpoint_dir / f"step_{global_step:06d}.pt"
                torch.save({
                    'step': global_step,
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss.item(),
                    'dataset_size': len(dataset),
                }, checkpoint_path)
                print(f"\nğŸ’¾ ä¿å­˜checkpoint: {checkpoint_path}")
        
        avg_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0
        elapsed = time.time() - start_time
        
        print(f"Epoch {epoch+1}/{args.max_epochs} - Loss: {avg_loss:.4f} - æ ·æœ¬: {len(dataset)} - Time: {elapsed/60:.1f}min")
        
        # ä¿å­˜æœ€æ–°
        latest_path = checkpoint_dir / "latest.pt"
        torch.save({
            'step': global_step,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
            'dataset_size': len(dataset),
        }, latest_path)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")
    print(f"   æ€»æ­¥æ•°: {global_step}")
    print(f"   æœ€ç»ˆæ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"   æ€»æ—¶é—´: {(time.time() - start_time)/3600:.2f} å°æ—¶")


def main():
    parser = argparse.ArgumentParser(description="DiTå¢é‡è®­ç»ƒ - åŠ¨æ€åŠ è½½æ–°æ•°æ®")
    
    # æ•°æ®
    parser.add_argument('--dataset-dir', type=str, required=True,
                        help='æ•°æ®é›†ç›®å½•ï¼ˆä¼šæŒç»­ç›‘æ§æ–°æ ·æœ¬ï¼‰')
    parser.add_argument('--output-dir', type=str, required=True,
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--min-samples', type=int, default=50,
                        help='å¼€å§‹è®­ç»ƒçš„æœ€å°æ ·æœ¬æ•°')
    parser.add_argument('--refresh-interval', type=int, default=60,
                        help='æ•°æ®é›†åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰')
    
    # æ¨¡å‹
    parser.add_argument('--model-size', type=str, default='small',
                        choices=['small', 'base', 'large'],
                        help='æ¨¡å‹å¤§å°')
    
    # è®­ç»ƒ
    parser.add_argument('--batch-size', type=int, default=8,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max-epochs', type=int, default=1000,
                        help='æœ€å¤§è®­ç»ƒè½®æ•°ï¼ˆå®é™…ä¼šæ ¹æ®æ•°æ®åŠ¨æ€è°ƒæ•´ï¼‰')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--num-workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--use-amp', action='store_true',
                        help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    
    # ä¿å­˜å’Œæ¨ç†
    parser.add_argument('--save-every', type=int, default=100,
                        help='æ¯Næ­¥ä¿å­˜ä¸€æ¬¡checkpoint')
    parser.add_argument('--inference-every', type=int, default=10,
                        help='æ¯Næ­¥æ¨ç†ä¸€æ¬¡ï¼ˆ0=ä¸æ¨ç†ï¼‰')
    parser.add_argument('--inference-samples', type=int, default=2,
                        help='æ¨ç†æ—¶ç”Ÿæˆçš„æ ·æœ¬æ•°')
    parser.add_argument('--inference-steps', type=int, default=50,
                        help='æ¨ç†æ—¶çš„é‡‡æ ·æ­¥æ•°')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("DiTå¢é‡è®­ç»ƒ - åŠ¨æ€æ•°æ®åŠ è½½")
    print("=" * 60)
    print(f"é…ç½®:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("=" * 60)
    
    train(args)


if __name__ == "__main__":
    main()
